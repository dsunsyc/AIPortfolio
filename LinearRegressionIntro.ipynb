{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegressionIntro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPSN95Oj7Qs3vVItiV+06o8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsunsyc/AIProjects/blob/master/LinearRegressionIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhkAYTmQVUni",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "Linear Regression is a powerful tool that finds a relationship between data points in the form of a best fitting curve. Using the relationship output by the learning algorithm, we are able to predict and estimate values for data that has never been encountered before. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLywMOegou6q",
        "colab_type": "text"
      },
      "source": [
        "This notebook is aimed to provide an introduction to Linear Regression. As such, it's not required to have previous knowledge of any ML algorithms, but having some experience with multivariable calculus and linear algebra would be useful!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec7j2akKafVB",
        "colab_type": "text"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYykfkM42R64",
        "colab_type": "text"
      },
      "source": [
        "We'll need to import some libraries to allow us to easily implement linear regression with gradient descent from scratch. \n",
        "\n",
        "1. First we'll need Numpy, as it'll allow us to perform matrix multiplication and other linear algebra operation. \n",
        "2. Pyplot from MatPlotLib will allow us to visualize and plot graphs about our model.\n",
        "3. We'll use Pandas to read our data from a csv file stored on a github repo.\n",
        "4. Seaborn will be used on top of pyplot to create some more visually aesthetic plots. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THFNeopgaeoo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1df36654-7c69-44dc-97b8-24a9f638f6d9"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "URL = \"https://raw.githubusercontent.com/dsunsyc/AIProjects/master/Abalone/abalone.data.csv\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo7kKnFFrI6o",
        "colab_type": "text"
      },
      "source": [
        "# **1.0 Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGN7nhCxXp-Q",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Intuition\n",
        "Say you live right next to an ice cream shop. It seems like more people buy ice cream when the day is hot. You decide to record the peak temperature of the day, along with the number of customers you see going into the ice cream shop and end up with the following data. \n",
        "\n",
        "Temp ($^{\\circ}$C) | Num. Customers \n",
        "-- | --\n",
        "15 | 3\n",
        "35 | 9\n",
        "25 | 6\n",
        "20 | 5\n",
        "30 | 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1IEoe7vUbFO",
        "colab_type": "text"
      },
      "source": [
        "From a glance it seems obvious that a relationship exists between the day's heat and the number of customers, but it's hard to come up with a detailed idea of their correlation. \n",
        "\n",
        "Plotting the dataset, for low enough dimensions, is an easy way to check for a relationship."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tC7mICBaYr1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "d3287fc6-9927-44f0-a36f-5573e8fa525a"
      },
      "source": [
        "temps = [15, 35, 25, 20, 30]\n",
        "customers = [3, 8, 6, 5, 5]\n",
        "sns.set_style(\"darkgrid\")\n",
        "df = pd.DataFrame({\"Temperature\": temps, \"Customers\": customers})\n",
        "ax = sns.lmplot(x=\"Temperature\", y=\"Customers\", data=df, fit_reg=False)\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kniib8u5gTqh",
        "colab_type": "text"
      },
      "source": [
        "As such, it's pretty obvious that a linear relationship does exist between the heat of day and number of customers. If we use Seaborn's built in function for fitting and plotting a regression line, we can find a well fitting line for prediction purposes. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbOq6P_whr8Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "bd86cbfc-ebc8-49ca-88f1-1e492e5ccfd1"
      },
      "source": [
        "ax = sns.lmplot(x=\"Temperature\", y=\"Customers\", data=df)\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNsWEbDDilFv",
        "colab_type": "text"
      },
      "source": [
        "The line of best fit that lmplot found allows us now to predict the amount of frozen dairy consumers given a certain temperature. For example, if the peak temperature was 32.5 $^{\\circ}$C, we would estimate seeing around 7 ice cream shop customers! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6wYac-Zlppn",
        "colab_type": "text"
      },
      "source": [
        "But how do we actually find the best model for our data? We use Linear Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30E3IdZLmAUX",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Evaluation and Cost Function\n",
        "Linear Regression, like many other learning algorithms, aims to minimize a cost function over a dataset by tweaking certain parameters (more on that in section 1.4). Intuitively, a good model would have as little deviation as possible to the given data. For instance, even though we observed only 5 customers when the temperature was 30 $^{\\circ}$C, our model from above would predict to see ~6 people instead. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U2AhkRPpIl9",
        "colab_type": "text"
      },
      "source": [
        "We can represent the **cost** of a single training example to be $(predictVal - trueVal)^2$, with the square to account for negative values of $predictVal - trueVal$. We take the average cost of each point in the dataset and that output would be our cost function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N58-y3BSq_ws",
        "colab_type": "text"
      },
      "source": [
        "### Mathematically:\n",
        "\n",
        "### $J(\\theta) = \\frac{1}{2m}*$$\\sum_{i=1}^{m} (h_\\theta(x_i) - y_i)^2$,\n",
        "\n",
        "where \n",
        "*   $J(\\theta)$ represents the cost function\n",
        "*   $\\theta$ represents the parameters of our model\n",
        "*   $m$ is the number of points we have in our dataset (ice cream example would have $m=5$)\n",
        "*   $h_\\theta(x_i)$ represents our model's predicted value of the $i$th point in our dataset \n",
        "*   $y_i$ represents the true value of the $i$th point in our dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IpZxYybaswd",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Prediction\n",
        "The task of prediction is actually quite simple. Going back to the ice cream example, our best fitting line was about $y = .2x + .4$, which would represent our hypothesis ($h_\\theta()$). But we have to find a way to represent that equation in our model. In that example, our model only predicted on one variable, the temperature of the day. However, for a more complex dataset, you can predict based on temperature, whether or not a sale is happening, the month of the year, etc. All those features take up their own variables, and thus our input to the model is a vector. We can extract all the variables into a vector called $x_i$ and all the coefficients that represent our model into vector $\\theta$. \n",
        "\n",
        "$\\theta$ in our ice cream example was [.4, .2] which we feed vector [$x_0$, $x_1$] in to predict our value. $x^0$ should just be thought of as the y-intercept variable, and is always 1. So even when our model just predicts on one variable, $x$ is still a vector with the first value being 1. Putting this together, we can represent the model by vector [.4, .2], and when given input [1, $x_1$] calculates the value as a dot product of the two vectors, $.4*1 + .2*x^1$. Dot product of vector [$v_1, v_2, ..., v_n$] and [$u_1, u_2, ..., u_n$] is the sum of all pair products: $v_1*u_1 + v_2*u_2 + ... + v_n*u_n$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV6szVyOqLOy",
        "colab_type": "text"
      },
      "source": [
        "The code below represents predicting on our ice cream example. You can try out different values of theta, as well as different values of the input to see how it affects the prediction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLyF5F-Rp22m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66d4a83b-1fd9-4f2d-f6cc-400802fd0f81"
      },
      "source": [
        "input = 17.5\n",
        "theta = np.array([.4, .2])\n",
        "x = np.array([1, input])\n",
        "\n",
        "pred = x.dot(theta)\n",
        "print(\"The predicted number of customers given temp \" + str(input) + \" is: \" + str(pred))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The predicted number of customers given temp 17.5 is: 3.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FiDbyF1bPKI",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJilTK2pXIFk",
        "colab_type": "text"
      },
      "source": [
        "Now that we have a well defined cost function, we can have our model learn to fit the data. As stated above, the objective of our learning is to minimize the cost of our parameters on the given data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpbKo_kia1HT",
        "colab_type": "text"
      },
      "source": [
        "We use gradient descent to iteratively have our values of $\\theta$ converge to their optimal values. As hinted by the name, gradient descent is a process where we aim to reach the minimum cost value for our parameter $\\theta$ based on the gradient (also known as the slope) of the cost function. \n",
        "\n",
        "![alt text](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png)\n",
        "Image source: http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/\n",
        "\n",
        "Following our graph from above, if our $\\theta_0$ was initialized to the rightmost side of the graph, we notice that the slope at that point is steeply positive (positive slope of large magnitute). Since we're trying to arrive at a minimum, we update $\\theta_0$ to go in the direction opposite to the slope. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XUO3XJYoEtA",
        "colab_type": "text"
      },
      "source": [
        "## Mathematically:\n",
        "\n",
        "while not converged:\n",
        "> for j = 1:n:\n",
        ">> $temp_j = \\theta_j - \\alpha*\\frac{\\delta}{\\delta\\theta_j}J(\\theta)$\n",
        "\n",
        "> for j = 1:n:\n",
        ">> $\\theta_j = temp_j$\n",
        "\n",
        "\n",
        "\n",
        "*   $n$ represents the total number of weights in $\\theta$ \n",
        "*   $\\alpha$ represents the learning rate, a constant that dictates how large a step taken per iteration would be\n",
        "*   $\\frac{\\delta}{\\delta\\theta_j}J(\\theta)$ represents the partial derivative of the cost function with respect to $\\theta_j$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBpIXqNHwDY9",
        "colab_type": "text"
      },
      "source": [
        "### Let's try to apply what we learned on a real dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1X-YxJWrkag",
        "colab_type": "text"
      },
      "source": [
        "# 2.0 Applying to a Real Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiCpyV5KUdBy",
        "colab_type": "text"
      },
      "source": [
        "Scientists have been researching a type of shellfish called abalone and have accumulated a large set of measurements (including sex, length, diameter) for the individuals they have found. Abalone, like trees, require a meticulous and time consuming process of counting rings on their shells to determine their age. The scientists speculate that there is a relationship to be found among those physical traits and the age of the abalone. Let's try and find said relation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZJlIcfO66qi",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Loading the Dataset and Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_r3fLyhwYSa",
        "colab_type": "text"
      },
      "source": [
        "First, we have to obtain the dataset that we'd be working with. The raw data is stored in the URL defined in setup. It's always a good idea to shuffle the dataset before you split it or train with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmtTKg6RVPjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "6a749062-0e28-4dba-9860-c096de6370a5"
      },
      "source": [
        "df = pd.read_csv(URL)\n",
        "df = df.sample(frac=1).reset_index(drop=True).to_numpy() #4176x9\n",
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['F', 0.45, 0.345, ..., 0.095, 0.135, 9],\n",
              "       ['F', 0.675, 0.52, ..., 0.3055, 0.37, 9],\n",
              "       ['M', 0.75, 0.55, ..., 0.397, 0.445, 11],\n",
              "       ...,\n",
              "       ['F', 0.625, 0.52, ..., 0.35100000000000003, 0.375, 11],\n",
              "       ['M', 0.465, 0.355, ..., 0.09699999999999999, 0.1395, 8],\n",
              "       ['F', 0.515, 0.405, ..., 0.1405, 0.177, 10]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGajnaf-xGp3",
        "colab_type": "text"
      },
      "source": [
        "Now we have our numpy array containing all the data from the abalone. \n",
        "Each row represents a different abalone, and the columns represent different measurements:\n",
        "\n",
        "0. Sex | 1. Length (mm) | 2. Diameter (mm) | 3. Height (mm) | 4. Whole weight (g) | 5. Schucked Weight (g) | 6. Viscera Weight (g) | 7. Shell Weight (g) | 8. Rings (Num Rings Correlates to Age)\n",
        "-- | -- | -- | -- | -- | -- | -- | -- | -- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o-4svDnyYLu",
        "colab_type": "text"
      },
      "source": [
        "We want to have all values in the array be numerical, so we map all the sexes to integer values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phpu7XOUyi5r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c4c5f8d2-4a99-4f43-b9ac-7519680169d3"
      },
      "source": [
        "sex_to_int = {'F':0, 'M':1, 'I':2}\n",
        "convert = lambda x: sex_to_int[x]\n",
        "sex_col = df[:,0]\n",
        "vfunc = np.vectorize(convert)\n",
        "df[:,0] = vfunc(sex_col)\n",
        "df[0:5,:]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0.45, 0.345, 0.12, 0.4165, 0.1655, 0.095, 0.135, 9],\n",
              "       [0, 0.675, 0.52, 0.175, 1.494, 0.7365, 0.3055, 0.37, 9],\n",
              "       [1, 0.75, 0.55, 0.18, 1.893, 0.9420000000000001, 0.397, 0.445, 11],\n",
              "       [2, 0.41, 0.3, 0.1, 0.282, 0.1255, 0.057, 0.0875, 7],\n",
              "       [2, 0.275, 0.2, 0.055, 0.0925, 0.038, 0.021, 0.026000000000000002,\n",
              "        4]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNGLhclYzTTS",
        "colab_type": "text"
      },
      "source": [
        "Next, we split up our data into 2 sets, training and test. We train our model on the training set, and then measure performance on the test set. A generally accepted ratio of training : test data is 7 : 3. \n",
        "\n",
        "We'll also cast all the data in the matrix to be type np.float64. For y_train and y_test, it's important to reshape the data into a vector of shape (m, 1), which is done by the reshape() function, in order to properly do matrix operations. The -1 in the argument means that the function will infer what size that will be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB7tg0tww34a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = df[:2923, :8].astype(np.float64)\n",
        "test = df[2923:, :8].astype(np.float64)\n",
        "y_train = df[:2923, 8].astype(np.float64).reshape((-1,1))\n",
        "y_test = df[2923:, 8].astype(np.float64).reshape((-1,1))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIZI8hlE2iIa",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 $\\theta$ Initialization\n",
        "\n",
        "We initialize $\\theta$ to be a vector of zeros. In order to be able to dot product theta with each example in our dataset, it has to be size (num features,1).\n",
        "We'll pass in n (num features) as a parameter to the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzEew65g2hcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_theta(n):\n",
        "  return np.zeros((n+1, 1))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLlEcVKHsFte",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6N_jfZyz14a",
        "colab_type": "text"
      },
      "source": [
        "An optimization technique to speed up the learning process is to use feature normalization. This centers all the training data at the origin as well as scaling it based on its standard deviation. \n",
        "\n",
        "**It's important to save the means and standard deviations of each feature, as when we try to predict using our model later on, we'd have to normalize those vectors as well.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0TUWgrKBalX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_normalize(X):\n",
        "    mean_arr = X.mean(axis=0)\n",
        "    std_arr = X.std(axis=0)\n",
        "    return ((X-mean_arr)/(std_arr), mean_arr, std_arr)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5NhkJx7sG8m",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Cost function\n",
        "\n",
        "We now implement our cost function, which returns a scalar value of average cost for our predictions on the dataset. It takes in parameters X, values, theta. X is the training set that we use, values are the labels or true values of those examples, and theta are the parameters (weights) for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5PODsTDCE4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cost(X, values, theta):\n",
        "    m,n = X.shape\n",
        "    assert(theta.shape == (n,1))\n",
        "    hypo = X.dot(theta)     #Computes the predicted value by finding the dot product between X and Theta\n",
        "    diff = (hypo-values)**2 #Computes the square error of each prediction with actual value\n",
        "    return diff.sum()/(2*m) "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4MnTtlD3zMy",
        "colab_type": "text"
      },
      "source": [
        "We can now test our cost function, and see it work on our dataset. \n",
        "\n",
        "We'll initialize our $\\theta$ and compute the cost that it has on our training dataset. \n",
        "\n",
        "If everything is set up properly, you should see this output a cost of ~55."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO34GOz94B9v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d4b81f6f-ea75-4968-8927-b5889897e7fa"
      },
      "source": [
        "train1, mean, std = feature_normalize(train)\n",
        "m, n = train1.shape\n",
        "theta = init_theta(n)\n",
        "weighted_train =  np.insert(train1, 0, 1, axis=1) #We're adding in the x0, the 1 in front of each training example\n",
        "cost(weighted_train, y_train, theta)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54.91806363325351"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkfV7q7WsI0Y",
        "colab_type": "text"
      },
      "source": [
        "## 2.5 Gradient descent/Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zETGnIHLS5va",
        "colab_type": "text"
      },
      "source": [
        "Gradient descent is the actual learning part of the algorithm. \n",
        "It takes in parameters X: our mxn matrix of examples, values: our mx1 array of true values, theta: the parameters for our model, iters: the number of iterations to run gradient descent for, alpha: the learning rate. It outputs a list of costs for each iteration. While gradient descent runs, it'll update (mutate) the parameter theta to achieve lower and lower costs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YgpsfFSNGTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(X, values, theta, iters, alpha):\n",
        "    m,n = X.shape \n",
        "    assert(theta.shape == (n, 1))\n",
        "    history = [None]*iters                     #initialize list of costs\n",
        "    for iter in range(iters):\n",
        "        hypo = X.dot(theta)                    #calculates prediction (mx1 matrix)\n",
        "        diff = hypo - values                   #finds difference (mx1 matrix)\n",
        "        grad = (alpha/m)*(X.T.dot(diff))       #computes the partial derivative with respect to each thetaj \n",
        "        theta -= grad                          #update theta\n",
        "        history[iter] = cost(X, values, theta) #record cost into history\n",
        "    return history"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRLqrEqlVSxI",
        "colab_type": "text"
      },
      "source": [
        "Now that we have a working gradient descent algorithm, we can try and learn better values for theta. \n",
        "\n",
        "Run the cell below and you should see a final value of hist being ~2.5, which is about a 20x improvement from our initial theta cost!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukf-Ata-VRws",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "562ae7af-b238-4399-a002-549fcfcb37e2"
      },
      "source": [
        "alpha = .1\n",
        "iters = 100\n",
        "# weighted_train.dot(theta).shape\n",
        "theta = init_theta(n)\n",
        "weighted_train = np.insert(train1, 0, 1, axis=1)\n",
        "hist = gradient_descent(weighted_train, y_train, theta, iters, alpha)\n",
        "hist[-1]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.6290607595192665"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lhgVHrwZA_u",
        "colab_type": "text"
      },
      "source": [
        "You can adjust the values for alpha (learning rate) and iters (the number of iterations gradient descent runs for). Generally you'll find a smaller alpha will result in smaller steps, and require more iterations to achieve low cost. However, although a larger alpha \"learns\" faster, it's also prone to overshooting, and can increase the cost/error!\n",
        "\n",
        "*Tip: Try adjusting alpha by factors of 3. .03 -> .1 -> .3 -> 1 -> 3 -> 10*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAZhl2OHsQnM",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMRZFvkI04fW",
        "colab_type": "text"
      },
      "source": [
        "We now have our model's $\\theta$ and history. A simple way to check if the model has learned properly is to plot the cost over iterations and see that it has decreased. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umEaXjb0ZSEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_cost(history):\n",
        "    iters = list(range(len(hist)))\n",
        "    df = pd.DataFrame({\"Error\": list(hist), \"Iters\": iters})\n",
        "    ax = sns.lmplot(x=\"Iters\", y=\"Error\", data=df, fit_reg=False)\n",
        "    plt.show()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q45mDzlr1P0g",
        "colab_type": "text"
      },
      "source": [
        "If everything has been done correctly, you should see the error (cost) converging to $<$5. If not, try and tweak around with learning rate (alpha) and your number of iterations (iters) in the gradient descent step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu7RJsmcaAFz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "e94643ba-127c-468f-99f3-192b022087b6"
      },
      "source": [
        "plot_cost(hist)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amq7xojix7hp",
        "colab_type": "text"
      },
      "source": [
        "Now that we have our model, we can use our testing set to validate our results. Since we trained our model on a separate subset of data, it's common to see that the cost for our test set is slightly higher than our training set. \n",
        "\n",
        "Since our model performed well on our training set, if we see a large difference in performance on our test set it usually means the model has overfit the training data, and is not generalized enough to fit new data. If our model didn't perform well on training data and test data, it usually means the model has underfit and is too simple to capture the complexity in the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URjFNpvy2MZ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "69bf636b-5a4f-4723-911e-5cbece62a7e5"
      },
      "source": [
        "weighted_test = np.insert(test, 0, 1, axis=1) #weighted_test (mxn+1)\n",
        "cost(weighted_test, y_test, theta)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.3931748703505"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D2NuGpNsK5D",
        "colab_type": "text"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "Prediction is as simple as computing the dot product between $\\theta$ and $x$. However, since we normalized our input vector, it's important to normalize any vector we try and predict, or the weights would be off. We also have to add the bias variable (prepending a 1 to the beginning of our input vector). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTSO8Tc6X-Z2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(example, theta, use_age, mean, std):\n",
        "    vector = (example - mean)/std\n",
        "    vector = np.insert(vector, 0, 1) #prepend one in front of the input vector\n",
        "    pred = vector.dot(theta) #compute dot product\n",
        "    return pred + 1.5 if use_age else pred #if we want age then we add 1.5 to result, else return number of rings"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-9uGHTZ4yhs",
        "colab_type": "text"
      },
      "source": [
        "Finally we can use our model to estimate the age of our abalone. Let's do some predictions!\n",
        "\n",
        "We'll use some sample data with some small tweaks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlTHw3lj5O_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "88d607e3-178d-44ec-d00f-b24dee139f79"
      },
      "source": [
        "ex1 = np.array(['F', .51, .28, .135, .63, .23, .142, .18]) #should predict near 9\n",
        "ex2 = np.array(['M', .44, .365, .125, .52, .23, .1, .16]) #should predict near 10\n",
        "ex3 = np.array(['I', .325, .245, .07, .16, .07, .025, .045]) #should predict near 7\n",
        "\n",
        "ex1[0] = sex_to_int[ex1[0]]\n",
        "ex2[0] = sex_to_int[ex2[0]]\n",
        "ex3[0] = sex_to_int[ex3[0]]\n",
        "\n",
        "ex1 = ex1.astype(np.float64)\n",
        "ex2 = ex2.astype(np.float64)\n",
        "ex3 = ex3.astype(np.float64)\n",
        "\n",
        "pred1 = predict(ex1, theta, True, mean, std)\n",
        "pred2 = predict(ex2, theta, True, mean, std)\n",
        "pred3 = predict(ex3, theta, True, mean, std)\n",
        " \n",
        "print(\"First prediction should be near 9: \" + str(pred1))\n",
        "print(\"Second prediction should be near 10: \" + str(pred2))\n",
        "print(\"Third prediction should be near 7: \" + str(pred3))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First prediction should be near 9: [11.04980174]\n",
            "Second prediction should be near 10: [10.65726584]\n",
            "Third prediction should be near 7: [8.06920319]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FjGlAiHb90P",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the predictions aren't quite perfect, but they can get pretty close!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikFfDJoBCn5H",
        "colab_type": "text"
      },
      "source": [
        "# 3.0 Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0HFLebXCtUS",
        "colab_type": "text"
      },
      "source": [
        "Linear Regression is powerful learning algorithm that finds relationships between data. Given a well constructed set of data, linear regression is able to output a fairly accurate model which can be used to predict values of never-before seen examples. However, Linear Regression isn't all powerful, often struggling to capture complex, non-linear relationships. However, when used on an appropriate dataset, Linear Regression is an incredibly reliable and efficient way to create a model of the data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55nmxBZuCzrz",
        "colab_type": "text"
      },
      "source": [
        "# 4.0 What's Next?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS3ilw7PC2st",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 Improving Model Accuracy\n",
        "Although our final model achieved decently accurate results, there are still ways to improve it. \n",
        "\n",
        "* Let gradient descent run for more iterations. However, as you can see with the cost plot, the cost eventually converges to a value where increasing the iterations has a negligible increase to accuracy.\n",
        "* Increase the number of training examples, so outliers and rare values have less of an impact.  \n",
        "* Feature engineering, which is trying to add features that capture more complex relationships between data.\n",
        "* Implement a polynomial linear regression, where instead of having the model output a hyperplane, the model could generate non-linear surfaces as well. This is a type of feature engineering. \n",
        "\n",
        "## 4.2 Moving On\n",
        "If you're learning ML for the first time, I'd recommend spending as much time as needed to solidify your understanding of linear regression. It's a powerful foundation to have, and many other learning algorithms share similar traits/parts. After that, you should explore other supervised learning algorithms, such Logistic Regression, before moving onto neural networks and unsupervised algorithms. Specifically, check out Andrew Ng's course on Machine Learning on Coursera, it's a great resource for beginners and makes things very easy to digest! https://www.coursera.org/learn/machine-learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmwbLd_Iitn2",
        "colab_type": "text"
      },
      "source": [
        "If you've made it this far, thank you so much for stepping through this colab notebook of mine! I made it mainly to try and improve my own understanding of this fundamental ML algorithm, and hopefully will have more to come! "
      ]
    }
  ]
}